{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не забудьте установить gym, если он у вас ещё не установлен\n",
    "# при помощи pip install gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# фиксируем сид\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша игра называется `CartPole-v0`, она как раз демонстрируется в документации к gym: https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")  # создадим среду\n",
    "env.seed(17)  # зафиксируем сид для воспроизводимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем изучить, какие параметры нам передаёт среда в качестве состояния, а какие действия м можем совершать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве состояния среды нам передаётся 4 числа, а совершать мы можем 2 дискретных действия (соответственно, приложить силу влево или вправо).\n",
    "\n",
    "Давайте обучим агента в виде полносвязной нейросети с механизмом Actor Critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4  # кол-во параметров состояния среды\n",
    "num_actions = 2  # кол-во возможных действий\n",
    "\n",
    "num_hidden = 64\n",
    "\n",
    "# создадим нейросеть при помощи Functional API\n",
    "# т.к. нам потребуется 2 выхода\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "common2 = layers.Dense(num_hidden, activation=\"relu\")(common)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common2)\n",
    "critic = layers.Dense(1)(common2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в качестве оптимизатора будем по стандарту использовать ADAM\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# в качестве функции потерь - Huber loss\n",
    "# она похожа на MSE, но оканчивается линейрой функцией, а не квадратичной\n",
    "# это придаст чуть больше стабильности нашему процессу обучения\n",
    "loss = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пришло время заставить нашу модель управлять игрой и обучать её.\n",
    "\n",
    "В каждом эпизоде мы будем производить один раунд игры, после чего обновлять веса модели в соответствии с историей действий и наград.\n",
    "\n",
    "Критерием успеха будем считать так называемый `running_reward` - как бы \"накопленную\" в ходе эпизодов награду (с затуханием). Когда она достигнет 195, будем считать, что модель натренировалась достаточно хорошо.\n",
    "\n",
    "Приступим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, running reward: 12.02\n",
      "Episode: 20, running reward: 27.79\n",
      "Episode: 30, running reward: 42.17\n",
      "Episode: 40, running reward: 49.42\n",
      "Episode: 50, running reward: 51.60\n",
      "Episode: 60, running reward: 69.63\n",
      "Episode: 70, running reward: 74.66\n",
      "Episode: 80, running reward: 80.90\n",
      "Episode: 90, running reward: 97.25\n",
      "Episode: 100, running reward: 126.91\n",
      "Episode: 110, running reward: 156.24\n",
      "Episode: 120, running reward: 173.80\n",
      "Episode: 130, running reward: 184.31\n",
      "Episode: 140, running reward: 185.70\n",
      "Episode: 150, running reward: 160.40\n",
      "Episode: 160, running reward: 139.71\n",
      "Episode: 170, running reward: 143.04\n",
      "Episode: 180, running reward: 165.11\n",
      "Episode: 190, running reward: 160.95\n",
      "Episode: 200, running reward: 167.98\n",
      "Episode: 210, running reward: 180.83\n",
      "Episode: 220, running reward: 161.18\n",
      "Episode: 230, running reward: 125.08\n",
      "Episode: 240, running reward: 124.88\n",
      "Episode: 250, running reward: 147.79\n",
      "Episode: 260, running reward: 167.19\n",
      "Episode: 270, running reward: 159.85\n",
      "Episode: 280, running reward: 153.01\n",
      "Episode: 290, running reward: 150.01\n",
      "Episode: 300, running reward: 140.21\n",
      "Episode: 310, running reward: 138.61\n",
      "Episode: 320, running reward: 136.07\n",
      "Episode: 330, running reward: 150.75\n",
      "Episode: 340, running reward: 158.10\n",
      "Episode: 350, running reward: 141.77\n",
      "Episode: 360, running reward: 127.40\n",
      "Episode: 370, running reward: 118.27\n",
      "Episode: 380, running reward: 122.90\n",
      "Episode: 390, running reward: 135.83\n",
      "Episode: 400, running reward: 148.35\n",
      "Episode: 410, running reward: 162.70\n",
      "Episode: 420, running reward: 177.66\n",
      "Episode: 430, running reward: 186.63\n",
      "Episode: 440, running reward: 190.81\n",
      "Episode: 450, running reward: 187.40\n",
      "Episode: 460, running reward: 189.56\n",
      "Episode: 470, running reward: 179.37\n",
      "Episode: 480, running reward: 149.60\n",
      "Episode: 490, running reward: 158.53\n",
      "Episode: 500, running reward: 157.21\n",
      "Episode: 510, running reward: 149.15\n",
      "Episode: 520, running reward: 148.94\n",
      "Episode: 530, running reward: 153.19\n",
      "Episode: 540, running reward: 167.90\n",
      "Episode: 550, running reward: 176.57\n",
      "Episode: 560, running reward: 172.46\n",
      "Episode: 570, running reward: 181.57\n",
      "Episode: 580, running reward: 188.97\n",
      "Episode: 590, running reward: 193.39\n",
      "Solved at episode 596!\n"
     ]
    }
   ],
   "source": [
    "# зададим discount-фактор для награды\n",
    "gamma = 0.995\n",
    "\n",
    "# как долго длится эпизод\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# минимальная машинная точность\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# выделим списки для хранения значений\n",
    "# вероятности действий, критика и награды\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    # сбрасываем состояние к изначальному и записываем его\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # проводим раунд игры, записывая работу нашей нейросети в GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # идём по раунду шагами\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render()  # для графического отображения окна с игрой\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # зная состояние, модель должна нам предсказать действие \n",
    "            # и поправку от критика\n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            # поправку критика сразу сохраним\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # выберем действие, исходя из предсказанных вероятностей\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            \n",
    "            # вероятность выбранного действия сохраняем в историю\n",
    "            # можно сразу и прологарифмировать, как того требует наш лосс\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # говорим среде выполнить выбранное действие\n",
    "            # и получаем от неё следующее состояние и награду\n",
    "            # если игра окончена, среда вернут done=True\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # записываем полученную награду\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # пересчитываем running_reward с затуханием\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # награда на каждом шаге игры (то, что нам и должен предсказать критик)\n",
    "        # определяется как суммарная награда от этого и всех последующих шагов (с затуханием)\n",
    "        # считаем их все в 1 проход и добавляем в returns\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # нормализуем их для повышения стабильности обучения\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # осталось посчитать значения функций потерь, чтобы затем осуществить шаг градиентного спуска\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # на каждом шаге считаем лосс для actor и critic\n",
    "            # у первого это просто награда (за вычетом поправки от критика), \n",
    "            # умноженная на логарифмическую вероятность действия\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # критик же оканчивается обычной регрессией:\n",
    "            # он должен уметь предсказывать ожидаемую награду на каждом шаге\n",
    "            critic_losses.append(\n",
    "                loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # теперь можем осуществить стандартный шаг градиентного спуска\n",
    "        # чтобы запустить его из одной точки, сложим все лоссы\n",
    "        # т.к. при суммировании градиент от каждого слагаемого отправится\n",
    "        # только в направлении данного слагаемого, это то, что нужно\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # перед следующим эпизодом очистим все списки\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    # каждые 10 эпизодов будем выводить running reward\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode: {episode_count}, running reward: {running_reward:.2f}\")\n",
    "\n",
    "    # если running reward стал больше 195, цель достигнута\n",
    "    if running_reward > 195:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "# саму среду в конце нужно закрыть\n",
    "# чтобы рендер прекратился и можно было дальше работать в питоне\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
